{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### <span style=\"color:red\">ACHTUNG:</span> Bitte zum Starten im Menü `Cell • Run All` ausführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deaktivieren der Warnungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fehlerkurven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abbildung 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafische Darstellung\n",
    "import matplotlib.pyplot as plt\n",
    "# Mathematik\n",
    "import numpy as np\n",
    "\n",
    "#ax = plt.subplot(111)\n",
    "dif = np.arange(-2.0, 2.0, 0.1)\n",
    "# Differenz\n",
    "plt.plot(dif, dif, label=r\"$(y - \\hat y)$\") \n",
    "# Absoluter Fehler\n",
    "#plt.plot(dif, abs(dif), label=r\"$|y - \\hat y|$\")  \n",
    "# Quadratischer Fehler\n",
    "plt.plot(dif, dif**2, label=r\"$(y - \\hat y)^2$\")\n",
    "# Platz für Legende festlegen\n",
    "leg = plt.legend(loc='upper center')\n",
    "# Raster\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# Raster Labels\n",
    "plt.xlabel('Differenz')\n",
    "plt.ylabel('Fehler')\n",
    "# Graphen anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradientenabsteig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafische Darstellung\n",
    "import matplotlib.pyplot as plt\n",
    "# Ganz wichtig, sonst wird der Plot nicht angezeigt\n",
    "%matplotlib inline\n",
    "\n",
    "# Identische Funktion\n",
    "def func_id(x):  \n",
    "        return x \n",
    "\n",
    "# Initialisierungen\n",
    "x = 0.2\n",
    "y = x\n",
    "# Startgewicht\n",
    "weight = -10.0\n",
    "# Für den Plot\n",
    "weights = []\n",
    "errors = []\n",
    "w_deltas = []\n",
    "\n",
    "# Tabelle erzeugen\n",
    "# Print Überschrift\n",
    "print(\"Input x = {:.6f}, Gewünschter Output y = {:.2f}\".format(x,y))\n",
    "print(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\"\n",
    "          .format('Iter', 'x','w','net i',\n",
    "                  'a','y_hat','y','E',\"E'\",'w delta'))\n",
    "# Fixe 120 Schritte\n",
    "for step in range(120):\n",
    "    # Net input berechnen    \n",
    "    net_i = weight * x\n",
    "    # Aktivierung (identische Funktion)    \n",
    "    activation = func_id(net_i)\n",
    "    # Errechneter Output    \n",
    "    y_hat = activation\n",
    "    # Quadratischer Fehler: Gewünschter - Errechneter Output    \n",
    "    error = 0.5*(y - y_hat)**2\n",
    "    # Gradient    \n",
    "    derivative = (-1.0)*x*(y - y_hat)\n",
    "    # Delta für Gewichtsanpassung    \n",
    "    w_delta = (-1)*derivative\n",
    "    # Daten für den Plot (weight,error)\n",
    "    weights.append(weight)\n",
    "    errors.append(error)\n",
    "    w_deltas.append(w_delta)\n",
    "    # Ausgabe der Änderungen alle 10 Schritte    \n",
    "    if step % 10 == 0:\n",
    "        print(\"{}\\t{}\\t{:6.2f}\\t{:5.2f}\\t{:5.2f}\"\n",
    "              \"\\t{:5.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\"\n",
    "          .format(step, x,weight,net_i,activation,y_hat,\n",
    "                  y,error,derivative,w_delta)) \n",
    "    # Dafür machen wir das alles: Gewichtsanpassung = Lernen        \n",
    "    weight += w_delta\n",
    "\n",
    "# Plot erzeugen\n",
    "# Figure und Subplot\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(weights, errors, label=\"Fehler\")\n",
    "ax1.plot(weights,w_deltas, label=\"w deltas\")\n",
    "# Titel\n",
    "ax1.set_title('Gradientenabstieg')\n",
    "# Legende\n",
    "legend = ax1.legend(loc='best', fancybox=True, framealpha=0.5)\n",
    "# Raster\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# Label\n",
    "plt.xlabel('Gewicht')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Backpropagation: Erste Anpassung - predict Methode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 6.2, Listing 6.3, Listing 6.5, Listing 6.6, Listing 6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für die mathematischen Operationen numpy importieren\n",
    "import numpy as np\n",
    "# NEW\n",
    "from sklearn.utils.validation import check_random_state\n",
    "# Grafische Darstellung\n",
    "import matplotlib.pyplot as plt\n",
    "# Ganz wichtig, sonst wird der Plot nicht angezeigt\n",
    "%matplotlib inline\n",
    "\n",
    "# Die Netzwerkklasse definieren\n",
    "class MLP(object):\n",
    "    \n",
    "    # Die identische Funktion\n",
    "    def func_id(self,x):\n",
    "        return x\n",
    "    \n",
    "    # Eine weltberühmte Aktivierungsfunktion: Die Sigmoide\n",
    "    def func_sigmoid(self,x):\n",
    "        # Wichtig: Nicht math.exp sondern np.exp wegen array \n",
    "        # Operationen verwenden\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_input_neurons=2,\n",
    "                 n_hidden_neurons=2,\n",
    "                 n_output_neurons=1,\n",
    "                 weights=None,\n",
    "                 # NEW \n",
    "                 eta=0.01,n_iterations=10,random_state=2, \n",
    "                 *args, **kwargs):\n",
    "        \"\"\" Initialisierung des Netzwerkes.\n",
    "        Wir verwenden eine fixe I-H-O Struktur für den Anfang \n",
    "             (Input-Hidden-Output)\n",
    "        Die Anzahl der Neuronen ist flexibel\n",
    "        Zusätzlich ist es möglich das Netzwerk mit Gewichten \n",
    "            zu initialisieren[W_IH,W_HO]\n",
    "        \"\"\"\n",
    "        # Anzahl der Neuronen pro Layer\n",
    "        self.n_input_neurons=n_input_neurons\n",
    "        self.n_hidden_neurons=n_hidden_neurons\n",
    "        self.n_output_neurons=n_output_neurons\n",
    "        # Gewichtsinitialisierung\n",
    "        self.weights = weights\n",
    "        W_IH=[]\n",
    "        W_HO=[]\n",
    "        # NEW Lernrate\n",
    "        self.eta = eta\n",
    "        # Iterationen\n",
    "        self.n_iterations=n_iterations\n",
    "        # NEW Zufallsgenerator\n",
    "        self.random_state = random_state\n",
    "        # NEW Erzeugung des Zufallsgenerators (RNG)\n",
    "        self.random_state_ = check_random_state(self.random_state)\n",
    "        # NEW Fehler beim fit       \n",
    "        self.errors=[]        \n",
    "        # Hier werden alle Daten zur Netzberechnung abgelegt\n",
    "        self.network=[]\n",
    "        # Input Layer + Bias Neuron: Spalten = n\n",
    "        #   et_i, a_i, o_i,d_i,delta_i\n",
    "        self.inputLayer = np.zeros((self.n_input_neurons+1,5))\n",
    "        # Bias Neuron Output ist immer +1\n",
    "        self.inputLayer[0] = 1.0 \n",
    "        # Den Input Layer zum Netwerk hinzufügen\n",
    "        self.network.append(self.inputLayer)\n",
    "        # Weights von Input Layer zum Hidden Layer W_IH \n",
    "        # Neuron: Zeile x Spalten: \n",
    "        #   Zeilen = # Hidden, Spalten = # Input\n",
    "        if weights:\n",
    "            W_IH = self.weights[0]    \n",
    "        else:\n",
    "            # NEW\n",
    "            W_IH = 2 * self.random_state_.random_sample(\\\n",
    "            (self.n_hidden_neurons+1,self.n_input_neurons+1)) - 1\n",
    "        self.network.append(W_IH)\n",
    "        # NEW Hidden Layer + Bias Neuron: \n",
    "        # Spalten = net_i,a_i,o_i,d_i,delta_i\n",
    "        self.hiddenLayer = np.zeros((self.n_hidden_neurons+1,5))\n",
    "        # Bias Neuron Output ist immer +1\n",
    "        self.hiddenLayer[0] = 1.0 \n",
    "        # Den Hidden Layer zum Netwerk hinzufügen\n",
    "        self.network.append(self.hiddenLayer)\n",
    "        # Weights von Hidden Layer zum Output Layer W_HO \n",
    "        # Neuron: Zeile x Spalten: \n",
    "        #   Zeilen = # Output, Spalten = # Hidden\n",
    "        if weights:\n",
    "            W_HO = self.weights[1]\n",
    "        else:\n",
    "            # NEW\n",
    "            W_HO = 2 * self.random_state_.random_sample(\\\n",
    "            (self.n_output_neurons+1,self.n_hidden_neurons+1)) - 1\n",
    "        self.network.append(W_HO)\n",
    "        # NEW Output Layer + Bias Neuron: \n",
    "        # Spalten = net_i,a_i,o_i,d_i,delta_i \n",
    "        self.outputLayer = np.zeros((self.n_output_neurons+1,5))\n",
    "        # Bias Neuron Output = 0, da nicht relevant. \n",
    "        # Nur wegen einheitlicher Indizierung vorhanden\n",
    "        self.outputLayer[0] = 0.0 \n",
    "        # Den Output Layer zum Netwerk hinzufügen\n",
    "        self.network.append(self.outputLayer)\n",
    "\n",
    "    def print(self):\n",
    "        print('Multi-Layer-Perceptron - Netzwerkarchitektur')\n",
    "        # Insgesamt 7 Stellen, mit drei Nachkommastellen ausgeben\n",
    "        np.set_printoptions(formatter={'float': lambda x: \"{0:7.3f}\".format(x)})\n",
    "        for idx,nn_part in enumerate(self.network):\n",
    "            print(nn_part)\n",
    "            print('----------v----------')  \n",
    "            \n",
    "    def predict(self,x):\n",
    "        \"\"\" Für Eingabe x wird Ausgabe y berechnet.\n",
    "        \"\"\"\n",
    "        ###############\n",
    "        # Input Layer\n",
    "        # Die inputs setzen: Alle Zeilen, Spalte 2\n",
    "        self.network[0][:,2] = x\n",
    "        ###############\n",
    "        # Hidden Layer\n",
    "        # Start von Zeile 1 wegen Bias Neuron auf Index Position 0\n",
    "        # net_j = W_ij . x\n",
    "        self.network[2][1:,0] = np.dot(self.network[1][1:,:],\\\n",
    "                                       self.network[0][:,2])\n",
    "        # a_j\n",
    "        self.network[2][1:,1] = self.func_sigmoid(\\\n",
    "                                       self.network[2][1:,0]) \n",
    "        # o_j\n",
    "        self.network[2][1:,2] = self.func_id(self.network[2][1:,1]) \n",
    "        # NEW der_j = o_j*(1-o_j) Ableitung für sigmoide\n",
    "        self.network[2][1:,3] = self.network[2][1:,2] \\\n",
    "                                * ( 1.0 - self.network[2][1:,2])\n",
    "        ###############\n",
    "        # Output Layer\n",
    "        # Start von Zeile 1 wegen Bias Neuron auf 0\n",
    "        # net_k = = W_jk . h\n",
    "        self.network[4][1:,0] = np.dot(self.network[3][1:,:],\\\n",
    "                                       self.network[2][:,2])\n",
    "        # a_k\n",
    "        self.network[4][1:,1] = self.func_sigmoid(\\\n",
    "                                       self.network[4][1:,0]) \n",
    "        # o_k\n",
    "        self.network[4][1:,2] = self.func_id(self.network[4][1:,1])\n",
    "        # NEW der_k = o_k*(1-o_k) Ableitung für sigmoide\n",
    "        self.network[4][1:,3] = self.network[4][1:,2] \\\n",
    "                                * ( 1.0 - self.network[4][1:,2])\n",
    "        # Rückgabe Output Vektor\n",
    "        return self.network[4][:,2]   \n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        \"\"\" Lernen          \n",
    "        \"\"\"         \n",
    "        # Gewichtsänderungen\n",
    "        delta_w_jk = []\n",
    "        delta_w_ij = []\n",
    "        # Fehler\n",
    "        self.errors = []\n",
    "        # Alle Iterationen\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Für alle Trainingsbeispiele\n",
    "            error = 0.0\n",
    "            #for xIdx,x in enumerate(X):\n",
    "            for x,y in zip(X,Y):    \n",
    "                #####################\n",
    "                # Vorwärtspfad\n",
    "                y_hat = self.predict(x)\n",
    "                # Differenz\n",
    "                diff = y - y_hat\n",
    "                # Quadratischer Fehler\n",
    "                error += 0.5 * np.sum(diff * diff)\n",
    "                \n",
    "                #####################\n",
    "                # Output Layer\n",
    "                # delta_k in der Output Schicht = der_k * diff\n",
    "                self.network[4][:,4] = self.network[4][:,3]*diff\n",
    "                \n",
    "                #####################\n",
    "                # Hidden Layer\n",
    "                # delta_j in der Hidden Schicht = \n",
    "                #   der_j * dot(W_kj^T,delta_k)\n",
    "                self.network[2][:,4] = \\\n",
    "                             self.network[2][:,3] * \\\n",
    "                             np.dot(self.network[3][:].T,\\\n",
    "                                    self.network[4][:,4])                 \n",
    "                \n",
    "                #####################\n",
    "                # Gewichtsdeltas von W_kj\n",
    "                # delta_w = eta * delta_k . o_j^T\n",
    "                delta_w_jk = self.eta * \\\n",
    "                             np.outer(self.network[4][:,4],\\\n",
    "                                      self.network[2][:,2].T )\n",
    "                # Gewichtsdeltas von W_ji\n",
    "                # delta_w = eta * delta_j . o_i^T\n",
    "                delta_w_ij = self.eta * \\\n",
    "                             np.outer(self.network[2][:,4],\\\n",
    "                                      self.network[0][:,2].T )\n",
    "                \n",
    "                #####################\n",
    "                # Gewichte anpassen\n",
    "                self.network[1][:,:] += delta_w_ij               \n",
    "                self.network[3][:,:] += delta_w_jk\n",
    "                \n",
    "            # Sammeln des Fehlers für alle Beispiele    \n",
    "            self.errors.append(error)\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\" Ausgabe des Fehlers  \n",
    "        Die im Fehlerarray gespeicherten Fehler als Grafik ausgeben  \n",
    "        \"\"\"         \n",
    "        # Figure Nummern Start\n",
    "        fignr = 1\n",
    "        # Druckgröße in inch\n",
    "        plt.figure(fignr,figsize=(5,5))\n",
    "        # Ausgabe Fehler als Plot\n",
    "        plt.plot(self.errors)\n",
    "        # Raster\n",
    "        plt.style.use('seaborn-whitegrid')  \n",
    "        # Labels\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Fehler')\n",
    "\n",
    "def main():\n",
    "    # Initialisierung der Trainingsbeispiele\n",
    "    X=np.array([[1.0,1.0,1.0],[1.0,0,1.0],[1.0,1.0,0],[1.0,0,0]])\n",
    "    Y=np.array([[0.0,0.0],[0.0,1.0],[0.0,1.0],[0.0,0.0]])\n",
    "    # Netzwerk initialisieren\n",
    "    nn = MLP(eta=0.03,n_iterations=40000,random_state=42)\n",
    "\n",
    "    # Trainieren des Netzes mit der fit Methode und Ausgabe nach dem Trainieren\n",
    "    nn.fit(X,Y)                        \n",
    "    nn.print()\n",
    "\n",
    "    # Error Ausgabe als Graph\n",
    "    nn.plot()\n",
    "\n",
    "    # Testen der Vorhersage des Trainings Datensatzes\n",
    "    print('Predict:')\n",
    "    for x,y in zip(X,Y):\n",
    "        print('{} {} -> {}'.format(x,y[1],nn.predict(x)[1:2]))\n",
    "    \n",
    "# Hauptprogramm\n",
    "# Achtung: Benötigt etwas länger, um einen Ausgabe zu produzieren\n",
    "main()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein \"fit\" Durchlauf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listing 6.9, Abbildung 6.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematik\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    # Trainingsdaten\n",
    "    X=np.array([[1.0,1.0,1.0]])\n",
    "    Y=np.array([[0.0,0.0]])  \n",
    "    \n",
    "    # Netzwerk initialisieren mit einer Iteration\n",
    "    nn = MLP(eta=0.03,n_iterations=1,printOn=False,random_state=42)\n",
    "\n",
    "    # Predict und Netzwerkarchitektur ausgeben\n",
    "    nn.predict(X[0])\n",
    "    nn.print()\n",
    "  \n",
    "    # Ein Beispiel lernen und Netzwerkarchitektur ausgeben\n",
    "    nn.fit(X,Y)                        \n",
    "    nn.print()\n",
    "    \n",
    "# Hauptprogramm\n",
    "main()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
